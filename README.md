# Sign Language Detection using YOLO-NAS

![Demo](path/to/demo.gif)

## Overview

This project utilizes the YOLO-NAS (You Only Look One-time Neural Architecture Search) model for real-time sign language detection through a webcam feed. The model is trained and fine-tuned on a custom dataset to accurately recognize sign language gestures.

## Features

- **Real-Time Detection**: Achieve high-performance sign language detection in real-time through a webcam feed.
- **YOLO-NAS Model**: Utilize the YOLO-NAS architecture, known for its efficiency and accuracy in object detection tasks.
- **Custom Dataset**: Train the model on a custom dataset tailored for sign language recognition.


<img width="476" alt="Screenshot 2024-02-05 at 2 59 57 AM" src="https://github.com/VarunRaj1920/Sign-language-detection/assets/101633273/d01fe3a7-bf21-4ef6-a249-df6d1e0eb70f">
<img width="505" alt="Screenshot 2024-02-05 at 2 59 38 AM" src="https://github.com/VarunRaj1920/Sign-language-detection/assets/101633273/1aee5b2c-3c7d-4514-b151-1baa079ad851">
